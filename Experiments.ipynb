{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/kasperipalkama/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/kasperipalkama/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/kasperipalkama/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus.reader.plaintext import PlaintextCorpusReader\n",
    "from nltk.corpus.reader.util import read_line_block\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "filename = 'subreddit_travel_may2015.csv'\n",
    "wd = os.getcwd()\n",
    "data_dir = os.path.join(wd, 'data')\n",
    "raw_data = pd.read_csv(os.path.join(data_dir, filename), header=None, skiprows=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess\n",
    "filename_pp = 'preprocessed.csv'\n",
    "data = raw_data[raw_data.iloc[:, 0] != '[deleted]']\n",
    "data.to_csv(os.path.join(data_dir, filename_pp), header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create corpus\n",
    "newcorpus = PlaintextCorpusReader(data_dir, fileids=filename_pp, para_block_reader=read_line_block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing corpus -- running time intentionally not optimized to make experimenting more convenient\n",
    "def stemming(word_list, stemmer=nltk.PorterStemmer()):\n",
    "    return [stemmer.stem(w) for w in word_list]\n",
    "\n",
    "def lemmatizing(word_list, lemmatizer=nltk.WordNetLemmatizer()):\n",
    "    return [lemmatizer.lemmatize(w) for w in word_list]\n",
    "\n",
    "def to_lower(word_list):\n",
    "    return [w.lower() for w in word_list]\n",
    "\n",
    "def remove_punctuation(word_list):\n",
    "    return [w for w in word_list if w.isalpha()]\n",
    "\n",
    "def remove_stopwords(word_list):\n",
    "    return [w for w in word_list if not w in stopwords.words('english')]\n",
    "\n",
    "\n",
    "def clean_sentences(word_list):\n",
    "    return remove_stopwords(\n",
    "        remove_punctuation(\n",
    "        to_lower(\n",
    "        lemmatizing(word_list))))\n",
    "\n",
    "sentences = list(map(lambda x: clean_sentences(x), newcorpus.sents()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'was', 'there', 'in', 'February', 'except', 'there', 'was', 'a', 'LOT', 'more', 'ice', '!']\n",
      "['wa', 'february', 'except', 'wa', 'lot', 'ice']\n",
      "['\"', 'I', 'get', 'what', 'you', 'said', ',', 'I', \"'\", 'm', 'just', 'letting', 'you', 'know', 'you', 'are', 'incorrect', '.\"']\n",
      "['get', 'said', 'letting', 'know', 'incorrect']\n",
      "['\"', 'Be', 'very', 'careful', 'of', 'being', 'hit', 'with', 'an', 'airport', 'exit', 'tax', '.']\n",
      "['careful', 'hit', 'airport', 'exit', 'tax']\n",
      "['I', 'don', \"'\", 't', 'know', 'what', 'the', 'situation', 'is', 'like', 'today', ',', 'but', 'as', 'recently', 'as', 'one', 'year', 'ago', 'it', 'was', 'about', '$', '56', '.\"']\n",
      "['know', 'situation', 'like', 'today', 'recently', 'one', 'year', 'ago', 'wa']\n",
      "['\"', 'I', 'don', \"'\", 't', 'know', 'how', 'into', 'boarding', 'you', 'are', ',', 'but', 'I', 'have', 'some', 'shock', 'pads', 'and', 'soft', 'wheels', 'to', 'put', 'on', 'my', 'board', 'to', 'smoothen', 'out', 'the', 'trip', '.']\n",
      "['know', 'boarding', 'shock', 'pad', 'soft', 'wheel', 'put', 'board', 'smoothen', 'trip']\n",
      "['We', 'have', 'cash', 'and', 'camping', 'equipment', 'we', 'are', 'bringing', ',', 'just', 'in', 'case', 'something', 'goes', 'south', '(', 'and', 'will', 'never', 'be', 'TOO', 'far', 'from', 'civilization', 'hopefully', ')', 'The', 'winding', 'roads', 'are', 'a', 'huge', 'plus', ',', 'on', 'a', 'board', 'they', 'are', 'very', 'fun', '.']\n",
      "['cash', 'camping', 'equipment', 'bringing', 'case', 'something', 'go', 'south', 'never', 'far', 'civilization', 'hopefully', 'winding', 'road', 'huge', 'plus', 'board', 'fun']\n",
      "['How', 'much', 'worse', 'are', 'they', '?']\n",
      "['much', 'worse']\n",
      "['We', 'can', 'deal', 'with', 'some', 'cracks', 'and', 'such', ',', 'but', 'crumbled', 'roads', 'or', 'gravel', 'would', 'force', 'us', 'to', 'turn', 'back', '.\"']\n",
      "['deal', 'crack', 'crumbled', 'road', 'gravel', 'would', 'force', 'u', 'turn', 'back']\n",
      "['They', \"'\", 're', 'insanely', 'expensive', '.']\n",
      "['insanely', 'expensive']\n",
      "['Wayyyyy', 'above', 'market', 'rate', '.']\n",
      "['wayyyyy', 'market', 'rate']\n"
     ]
    }
   ],
   "source": [
    "n_sentences = 10\n",
    "for raw_s, s in zip(newcorpus.sents()[0:n_sentences], sentences[0:n_sentences]):\n",
    "    print(raw_s)\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word2Vec\n",
    "model = Word2Vec(sentences, size=100, window=5, min_count=1, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(636280, 676150)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train(sentences=sentences, total_examples=model.corpus_count, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('cambodia', 0.9737130403518677),\n",
       " ('vietnam', 0.9609863758087158),\n",
       " ('spain', 0.9257575869560242),\n",
       " ('india', 0.9173445105552673),\n",
       " ('japan', 0.9119035005569458),\n",
       " ('italy', 0.911872148513794),\n",
       " ('germany', 0.9074152708053589),\n",
       " ('korea', 0.9055827260017395),\n",
       " ('southern', 0.8994117975234985),\n",
       " ('greece', 0.8949986696243286)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predict\n",
    "model.wv.similar_by_word('thailand')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wa_venv",
   "language": "python",
   "name": "wa_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
